{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alias():\n",
    "    alias_dict = defaultdict(list)\n",
    "    with open('../../resources/aliasCache') as f:\n",
    "        alias_set = [[k for k in x.strip().split(\"###\")  if k] for x in f.readlines()]\n",
    "        alias_set = [k for k in alias_set if len(k)>1]\n",
    "        for alias in alias_set:\n",
    "            for each_name in alias:\n",
    "                alias_dict[each_name].extend(alias)\n",
    "    return alias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annot(fname):\n",
    "    elem_dict = {}\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            elem = json.loads(line.strip())\n",
    "            docId = elem.pop('docId')\n",
    "            elem_dict[docId] = elem\n",
    "    return elem_dict\n",
    "def load_doc(dirname, annot_ids):\n",
    "    doc_dict = {}\n",
    "    for annot_id in annot_ids:\n",
    "        fname = dirname + annot_id + \".json\"\n",
    "        with open(fname) as f:\n",
    "            doc_elem = json.load(f)\n",
    "            doc_dict[annot_id] = doc_elem\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurences(named_entity_list, cluster_json, alias_dict):\n",
    "    ne_per_locations = defaultdict(list)\n",
    "    ne_per_clusters = defaultdict(list)\n",
    "    alter_names = defaultdict(set)\n",
    "    ne_type = {}\n",
    "    # check type, and merge person type with shared token.\n",
    "    for ne, occurences in named_entity_list.items():\n",
    "        ne_type[ne] = most_common([occ[1] for occ in occurences]) \n",
    "    for ne in sorted(named_entity_list, key=lambda k: len(named_entity_list[k]), reverse=True):\n",
    "        occurences = named_entity_list[ne]\n",
    "        # Merge aliases:\n",
    "        total_aliases = [alias_dict.get(k,[]) for k in named_entity_list.keys() if not k == ne]\n",
    "        total_aliases = [item for sublist in total_aliases for item in sublist]\n",
    "        alias_in_doc = set(alias_dict.get(ne, [])).intersection(total_aliases)\n",
    "        recorded = False\n",
    "        if alias_in_doc:\n",
    "            for ne2 in named_entity_list:\n",
    "                if ne == ne2:\n",
    "                    continue\n",
    "                elif set(alias_dict.get(ne2,[])).intersection(set(alias_dict.get(ne,[]))):\n",
    "                    if ne2 in ne_per_locations:\n",
    "                        alter_names[ne2].add(ne)\n",
    "                        alter_names[ne].add(ne2)\n",
    "                        ne_per_locations[ne2].extend([(occ[0], occ[2], occ[3]) for occ in occurences])\n",
    "                        recorded = True\n",
    "            if not recorded:\n",
    "                ne_per_locations[ne].extend([(occ[0], occ[2], occ[3]) for occ in occurences])\n",
    "        elif ne_type[ne] == 'PERSON':\n",
    "            for ne2 in named_entity_list:\n",
    "                if ne == ne2 or not ne_type[ne2]=='PERSON':\n",
    "                    continue\n",
    "                elif (ne.split()[-1])==(ne2.split()[-1]) and (not len(ne.split()) == len(ne2.split())):\n",
    "                    if ne2 in ne_per_locations:\n",
    "                        alter_names[ne2].add(ne)\n",
    "                        alter_names[ne].add(ne2)\n",
    "                        ne_per_locations[ne2].extend([(occ[0], occ[2], occ[3]) for occ in occurences])\n",
    "                        recorded = True\n",
    "                    elif ne in ne_per_locations:\n",
    "                        ne_per_locations[ne].extend([(occ[0], occ[2], occ[3]) for occ in occurences])\n",
    "                        recorded = True\n",
    "            if not recorded:\n",
    "                ne_per_locations[ne].extend([(occ[0], occ[2], occ[3]) for occ in occurences])\n",
    "        else:\n",
    "            ne_per_locations[ne].extend([(occ[0], occ[2], occ[3]) for occ in occurences])\n",
    "    cluster_used = []\n",
    "    # add mentions from coref clusters.\n",
    "    for cluster in cluster_json:\n",
    "        cluster_mention_loc = [(mention['sent_ind'], mention['token_ind'], mention['end_ind']-1) for mention in cluster]\n",
    "        for ne, ne_locs in ne_per_locations.items():\n",
    "            if not set(ne_locs).isdisjoint(cluster_mention_loc) and (not cluster in cluster_used):\n",
    "                ne_per_clusters[ne] = cluster\n",
    "                cluster_used.append(cluster)\n",
    "    ne_total = defaultdict(list)\n",
    "    for k in ne_per_locations:\n",
    "        ne_total[k].extend(ne_per_locations[k])\n",
    "        cluster_mention_loc = [(mention['sent_ind'], mention['token_ind'], mention['end_ind']-1) for mention in  ne_per_clusters.get(k, [])]\n",
    "        ne_total[k].extend(cluster_mention_loc)\n",
    "    return ne_total, alter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_dict = load_annot('../annotation/acl_mpqa_eval.json')\n",
    "doc_dict = load_doc('../doc/mpqa_data/', annot_dict.keys())\n",
    "alias_dict = load_alias()\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sent(sent, strength):\n",
    "    if sent == 'NotNegative':\n",
    "        return 'Positive'\n",
    "    if sent == 'NotPositive':\n",
    "        return 'Negative'\n",
    "total_pair=0\n",
    "label_count=0\n",
    "orig_ctn = Counter()\n",
    "label_counter = Counter()\n",
    "with open('./mpqa_new.json', 'w') as fout:\n",
    "    for doc_id, annot_elem in annot_dict.items():\n",
    "        doc_elem = doc_dict[doc_id]\n",
    "        output_list = []\n",
    "        ne_loc_dict, alter_names = find_occurences(doc_elem['named_entity'], doc_elem['cluster_json'], alias_dict)\n",
    "        doc_level_ne_loc = {}\n",
    "        sent_dict = {(d['holder'], d['target']): map_sent(d['sentiment'], d['strength']) for d in annot_elem['sentiments'] if not d['holder']==d['target']}\n",
    "        orig_ctn.update(sent_dict.values())\n",
    "        label_count += len(sent_dict.keys())\n",
    "        exist = 0\n",
    "        total_tokens = [item for sublist in doc_elem['text'] for item in sublist]\n",
    "        per_sent_token = [len(sublist) for sublist in (doc_elem['text'])]\n",
    "        for ne, ne_loc_list in ne_loc_dict.items():\n",
    "            new_ne_locs = set([])\n",
    "            for sent_ind, start_ind, end_ind in ne_loc_list:\n",
    "                prev_sent_token_sum = sum(per_sent_token[:sent_ind-1])\n",
    "                new_ne_locs.add((prev_sent_token_sum+start_ind-1, prev_sent_token_sum+end_ind-1))\n",
    "            doc_level_ne_loc[ne] = list(new_ne_locs)\n",
    "        for ent1, ent1_loc in ne_loc_dict.items():\n",
    "            for ent2, ent2_loc in ne_loc_dict.items():\n",
    "                label = 'Null'\n",
    "                total_pair+=1\n",
    "                if ent1 == ent2:\n",
    "                    continue\n",
    "                if (ent1, ent2) in sent_dict:\n",
    "                    label = sent_dict.pop((ent1, ent2))\n",
    "                else:\n",
    "                    for ent1_alt_name in alter_names.get(ent1, []):\n",
    "                        if (ent1_alt_name, ent2) in sent_dict:\n",
    "                            label = sent_dict.pop((ent1_alt_name, ent2))\n",
    "                        for ent2_alt_name in alter_names.get(ent2, []):\n",
    "                            if (ent1, ent2_alt_name) in sent_dict:\n",
    "                                label = sent_dict.pop((ent1, ent2_alt_name))\n",
    "                            if (ent1_alt_name, ent2_alt_name) in sent_dict:\n",
    "                                label = sent_dict.pop((ent1_alt_name, ent2_alt_name))\n",
    "                    for ent2_alt_name in alter_names.get(ent2, []):\n",
    "                        if (ent1, ent2_alt_name) in sent_dict:\n",
    "                            label = sent_dict.pop((ent1, ent2_alt_name))\n",
    "                ent1_sent = [k[0] for k in ent1_loc]\n",
    "                ent2_sent = [k[0] for k in ent2_loc]\n",
    "                joint_sent = set(ent1_sent).intersection(set(ent2_sent))\n",
    "                single_pair = {'holder':ent1, 'target':ent2, 'docid':doc_id,\n",
    "                               'label': label, 'token':total_tokens, \n",
    "                               'holder_index':doc_level_ne_loc[ent1], 'target_index': doc_level_ne_loc[ent2]}\n",
    "                fout.write(json.dumps(single_pair) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
